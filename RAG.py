# -*- coding: utf-8 -*-
"""CRQA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kmNxN1fWC1SA27y_FtQI_v4ur8l0d-oj

# **RAG Flow with open source and closed source models**
"""

from google.colab import drive
drive.mount('/content/drive')

!pip -q install langchain
!pip -q install bitsandbytes accelerate xformers einops
!pip -q install datasets loralib sentencepiece
!pip -q install pypdf
!pip -q install sentence_transformers
!pip install chromadb
!pip install openai
!pip install tiktoken
!pip install -U langchain-community
!wget -q https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.4.1/auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl
!pip install -qqq auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl --progress-bar off
!sudo apt-get install poppler-utils
!pip install langchain_groq
!pip install langchain_anthropic

import os
os.environ["OPENAI_API_KEY"] = ""
os.environ["GROQ_API_KEY"] = ""

import torch
import transformers
from langchain_community.document_loaders import PyPDFLoader, TextLoader , UnstructuredWordDocumentLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain, RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq
from langchain_anthropic import ChatAnthropic
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts.prompt import PromptTemplate
from langchain.chat_models import ChatOpenAI

# loaders = [
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/AMEX.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/ASX.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/EUREX.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/FOREX.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/INDEX.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/LIFFE.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/LSE.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/MGEX.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/nasdaq.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/nybot.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/nyse.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/OTCBB.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/SGX.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/TSX.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/TSXV.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/Usmf.txt"),
#      TextLoader("/content/drive/MyDrive/Finance_Data/Finance_Symbols/WCE.txt"),
#      PyPDFLoader("/content/drive/MyDrive/Finance_Data/List_of_business_and_finance_abbreviations.pdf"),
#      PyPDFLoader("/content/drive/MyDrive/Finance_Data/Financial Statements and Reporting.pdf")
# ]

loaders = [
     PyPDFLoader("/content/drive/MyDrive/Finance_Data/Finance_term_Explanation/List_of_business_and_finance_abbreviations.pdf"),
     PyPDFLoader("/content/drive/MyDrive/Finance_Data/Businees_Finance_statments_&_Reporting/Financial Statements and Reporting.pdf")
]

docs = []
for l in loaders:
    docs.extend(l.load())
len(docs)

text_splitter = CharacterTextSplitter(
    separator = "\n",
    chunk_size = 1000,
    chunk_overlap  = 200,
)
document_chunks=text_splitter.split_documents(docs)

# from langchain.embeddings import HuggingFaceEmbeddings
# embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-large-en')

embedding = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
docs=Chroma.from_documents(document_chunks ,embedding=embedding , persist_directory='./data')

"""# **For Open source embedding and Open source model use this chunk of code below:**"""

# from langchain.embeddings import HuggingFaceEmbeddings
# embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-large-en')

# from huggingface_hub import notebook_login
# import torch
# import transformers
# from auto_gptq import AutoGPTQForCausalLM
# from transformers import AutoTokenizer, AutoModelForCausalLM
# from transformers import pipeline , TextStreamer
# from langchain import HuggingFacePipeline

# from langchain.llms import OpenAI
# from langchain.embeddings.openai import OpenAIEmbeddings

# from langchain.prompts import PromptTemplate, ChatPromptTemplate

# import os
# import sys
# from transformers import AutoTokenizer, TextStreamer, pipeline
# DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"

# tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-rw-1b",
#                                           use_auth_token=True, )


# model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-rw-1b",
#                                              device_map='auto',
#                                              torch_dtype=torch.float16,
#                                              use_auth_token=True,
#                                               #load_in_8bit=True,
#                                               load_in_4bit=True,
#                                              #trust_remote_code=True
#                                              )

# model_name_or_path = "TheBloke/Llama-2-13B-chat-GPTQ"
# model_basename = "model"

# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)

# model = AutoGPTQForCausalLM.from_quantized(
#     model_name_or_path,
#     revision="gptq-4bit-128g-actorder_True",
#     model_basename=model_basename,
#     use_safetensors=True,
#     trust_remote_code=True,
#     inject_fused_attention=False,
#     device=DEVICE,
#     quantize_config=None,
# )

# streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

# DEFAULT_SYSTEM_PROMPT = """
# You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
# """.strip()


# def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:
#     return f"""
# [INST] <<SYS>>
# {system_prompt}
# <</SYS>>

# {prompt} [/INST]
# """.strip()

# SYSTEM_PROMPT = "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer."

# template = generate_prompt(
#     """
# {context}

# Question: {question}
# """,
#     system_prompt=SYSTEM_PROMPT,
# )

# text_pipeline = pipeline(
#     "text-generation",
#     model=model,
#     tokenizer=tokenizer,
#     max_new_tokens=1024,
#     temperature=0,
#     top_p=0.95,
#     repetition_penalty=1.15,
#     streamer=streamer,
# )

"""# **Below are the open source and closed source models which were using by API**"""

# llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={"temperature": 0.7})
llm = ChatOpenAI(model = "gpt-4o", temperature=0, openai_api_key=os.getenv("OPENAI_API_KEY"))
# llm = ChatOpenAI(model = "gpt-3.5-turbo", temperature=0, openai_api_key=os.getenv("OPENAI_API_KEY"))
# llm = ChatGroq(temperature=0, model_name="llama3-70b-8192", api_key=os.getenv("GROQ_API_KEY"))
# llm = ChatGroq(temperature=0, model_name="mixtral-8x7b-32768", api_key=os.getenv("GROQ_API_KEY"))
# llm = ChatGroq(temperature=0, model_name="gemma-7b-it", api_key=os.getenv("GROQ_API_KEY"))
# llm = ChatGroq(temperature=0, model_name="llama3-8b-8192", api_key=os.getenv("GROQ_API_KEY"))
# llm = ChatAnthropic(model="claude-3-haiku-20240307", api_key=os.getenv("ANTHROPIC_API_KEY"))

#this below template is used with open source model genration
# prompt = PromptTemplate(template=template, input_variables=["context", "question"])

template = """
You are an expert in financial terms. Provide detailed and accurate information about the given topics. Use professional language and explain any complex terms clearly. Explain the financial terms user ask for.
Above are some example on how to answer the question. The answer should be short and to the point. Acording to above examples answer any question about any stock symbole in your knowledge.
just elaborate the term ask by the user according to given context. don't add any irrelivent information.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Context:\""
{context}
\""

Question:\""
{question}
\""

Helpful and Complete Answer:"""

qa_prompt= PromptTemplate.from_template(template)

memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key="result")

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=docs.as_retriever(search_kwargs={"k": 2}),
    return_source_documents=True,
    chain_type_kwargs={"prompt": qa_prompt},
    memory= memory
)

def chat_function(question):
    result = qa_chain.invoke(question)
    return print(result["result"])

results = chat_function("Explain this term Asset Allocation?")

results = chat_function("Explain this term Bond?")

results = chat_function("Explain this term Volatility?")

results = chat_function("Explain this term Capital Gains?")

results = chat_function("Explain this term Blockchain?")